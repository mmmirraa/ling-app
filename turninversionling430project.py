# -*- coding: utf-8 -*-
"""TurnInVersionLing430Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWHSRrbyDRBzXwesm76NP6udBw0vCLWn

# Welcome to Angelica, Samaya, and Mira's LING 430 Final Project!

This project aims to produce an interactive verb identifier that identifies the kind of past tense and the kind of conjugation

Throughout the project we utilize the SpaCy package, particularly the trained model pt_core_news_sm

Before running this project, ensure you have run the following in your terminal:

-m spacy download pt_core_news_sm

**Our next steps for this project, before submitting for final review will be the following: creating an effective regex based function for automatically conjugating, and then identifying the conjugation of novel regular verbs.**
**If possible we may add present or future tense conjugations as well**

# Set-Up System
"""


# MAJOR NOTE: Majority of this code was used as a reference, and is not actually
# directly utilized in the development of the website

import os
import re
import nltk
from nltk.tokenize import word_tokenize
import pandas as pd
import spacy

# Load spacy model (make sure you've run: python -m spacy download pt_core_news_sm)
nlp = spacy.load("pt_core_news_sm")

# Path to your data folder inside the repo
ROOT_DIR = os.path.dirname(__file__)   # folder where this script lives
DATADIR = os.path.join(ROOT_DIR, "data")   # folder containing verbs.csv, bfamdl/, etc.

print("Using data directory:", DATADIR)

"""# Read Data

"""

#  Opens contents of the training dialougues, which are the Portuguese- ca /
#C-ORAL-BRASIL / - files from TalkBank specifically the bfamdl files which focus on everyday familial conversation.
def read_bfamdl_files(base_path = "bfamdl", prefix = "bfamdl", start = 1, end = 14):
  """
    This function iteratively reads through our first 14 bfamld transcripts, which are our training data.
  """
  combined_text = "" #<- creates a massive string with our transcripts
  for i in range(start, end + 1):
    file_name = f"{base_path}/{prefix}{i:02d}.cha"
    with open(os.path.join(DATADIR, file_name), 'r', encoding='utf-8') as f:
                combined_text += f.read() + "\n"
  return combined_text

def dialogue_cleaner (lines):
  """
    This function cleans up the dialogue read from our conversation transcripts removing auxillary information
    as well as isolating lines with dialogue, and then cleaning them up by removing time signatures and any extra punctuation.

  """
  dialogue = [] #<- the lines of the transcrips that contain transcripts of portuguese
  cleaned_dialogue_lines = [] #<- the lines of the transcript that are iteratively being cleaned
  complete_cleaned_dialogue_lines = [] #<- a massive list of clean dialogue lines
  for line in lines:
    if line.startswith("*PAR"):
      dialogue.append(line)

  for line in dialogue: #<- the following lines clean up the dialogue using regex
    cleaned_dialogue_lines = re.sub(r"^(\*PAR\d:)", "", line)
    cleaned_dialogue_lines = re.sub(r"\x15\d+_\d+\x15", "", cleaned_dialogue_lines)
    cleaned_dialogue_lines = re.sub(r"\n","", cleaned_dialogue_lines)
    cleaned_dialogue_lines = cleaned_dialogue_lines.strip()
    cleaned_dialogue_lines = cleaned_dialogue_lines.lower()
    complete_cleaned_dialogue_lines.append(cleaned_dialogue_lines)

  full_text = " ".join(complete_cleaned_dialogue_lines) #<- a version of complete_cleaned_dialogue_lines that is a string

  return full_text

cleaned_dialogue = dialogue_cleaner(read_bfamdl_files().split("\n"))

#  In: n, num of most popular verbs you want
#  Out: list of top n most common verbs in the corpus
#  This function simply creates a list of ordered verbs

def get_top_verbs(n):
  """
  This function identifies the most popular verbs in our corpus using the list of most popular lemmas from the verbs.csv file (provided by the creators of the bfamld corpus)
  """
  verb_freq = pd.read_csv(os.path.join(DATADIR, "verbs.csv"))
  top_n_verbs = verb_freq['Lemma'].head(n).to_list()

  return top_n_verbs

top_verbs = get_top_verbs(58) #<- the verbs with more than 100 instances in the complete corpus (as lemmas)

#THIS CODE: Creates a dataframe for each regular verb of all of its conjugations
columns = [
    'Infinitive',
    '1SG-PSTSimple',
    '2SG-InfPSTSimple',
    '2SG-ForPSTSimple',
    '3SG-PSTSimple',
    '1PL-PSTSimple',
    '2PL-InfPSTSimple',
    '2PL-ForPSTSimple',
    '3PL-PSTSimple',
    '1SG-PSTImperfect',
    '2SG-InfPSTImperfect',
    '2SG-ForPSTImperfect',
    '3SG-PSTImperfect',
    '1PL-PSTImperfect',
    '2PL-InfPSTImperfect',
    '2PL-ForPSTImperfect',
    '3PL-PSTImperfect'
]

verb_endings = pd.DataFrame(columns=columns)

ar_regex = r""

def get_ar_endings(verb):
  """
  This function identifies all of the conjugations of regular ar endings
  """
  short = verb[:-2]   # removes -ar
  return {
        'Infinitive': verb,
        '1SG-PSTSimple': short + 'ei',
        '2SG-InfPSTSimple': short + 'aste',
        '2SG-ForPSTSimple': short + 'ou',
        '3SG-PSTSimple': short + 'ou',
        '1PL-PSTSimple': short + 'amos',
        '2PL-InfPSTSimple': short + 'astes',
        '2PL-ForPSTSimple': short + 'aram',
        '3PL-PSTSimple': short + 'aram',
        '1SG-PSTImperfect': short + 'ava',
        '2SG-InfPSTImperfect': short + 'avas',
        '2SG-ForPSTImperfect': short + 'ava',
        '3SG-PSTImperfect': short + 'ava',
        '1PL-PSTImperfect': short + 'ávamos',
        '2PL-InfPSTImperfect': short + 'áveis',
        '2PL-ForPSTImperfect': short + 'avam',
        '3PL-PSTImperfect': short + 'avam'
    }

def get_er_endings(verb):
  """
  This function identifies all of the conjugations of regular er endings
  """
  short = verb[:-2]   # removes -er
  return {
        'Infinitive': verb,
        '1SG-PSTSimple': short + 'i',
        '2SG-InfPSTSimple': short + 'este',
        '2SG-ForPSTSimple': short + 'eu',
        '3SG-PSTSimple': short + 'eu',
        '1PL-PSTSimple': short + 'emos',
        '2PL-InfPSTSimple': short + 'estes',
        '2PL-ForPSTSimple': short + 'eram',
        '3PL-PSTSimple': short + 'eram',
        '1SG-PSTImperfect': short + 'ia',
        '2SG-InfPSTImperfect': short + 'ias',
        '2SG-ForPSTImperfect': short + 'ia',
        '3SG-PSTImperfect': short + 'ia',
        '1PL-PSTImperfect': short + 'íamos',
        '2PL-InfPSTImperfect': short + 'íeis',
        '2PL-ForPSTImperfect': short + 'iam',
        '3PL-PSTImperfect': short + 'iam'
    }

def get_ir_endings(verb):
  """
  This function identifies all of the conjugations of regular ir endings
  """
  short = verb[:-2]   # removes -ir
  return {
      'Infinitive': verb,
      '1SG-PSTSimple': short + 'i',
      '2SG-InfPSTSimple': short + 'iste',
      '2SG-ForPSTSimple': short + 'iu',
      '3SG-PSTSimple': short + 'iu',
      '1PL-PSTSimple': short + 'imos',
      '2PL-InfPSTSimple': short + 'istes',
      '2PL-ForPSTSimple': short + 'iram',
      '3PL-PSTSimple': short + 'iram',
      '1SG-PSTImperfect': short + 'ia',
      '2SG-InfPSTImperfect': short + 'ias',
      '2SG-ForPSTImperfect': short + 'ia',
      '3SG-PSTImperfect': short + 'ia',
      '1PL-PSTImperfect': short + 'íamos',
      '2PL-InfPSTImperfect': short + 'íeis',
      '2PL-ForPSTImperfect': short + 'iam',
      '3PL-PSTImperfect': short + 'iam'
    }

"""#"""

#The following code (as a whole cell) creates the dataframe verb_totals which is our dictionary of our top verbs (top 58, more than 100 instances)
#This is something we are activelly hoping to expand in the final project submission
irregular_verbs_list = ['ser', 'estar', 'ter', 'poder', 'ir', 'fazer', 'dar', 'dizer',
                   'querer', 'saber', 'ver', 'vir', 'pôr']

# In: a list of verbs, ideally from top_verbs
# Returns a dictionary of {verb: ending_type} for each verb

def get_verb_type(verb_list):
  """
  Identifies regular and irregular verbs that are in our top verbs
  """
  known_verbs = {}
  for verb in verb_list:
    if verb not in irregular_verbs_list:
      if verb.endswith('ar'):
        known_verbs[verb] = 'ar'
      elif verb.endswith('er'):
        known_verbs[verb] = 'er'
      elif verb.endswith('ir'):
        known_verbs[verb] = 'ir'
      else: # this marks pôr irregular
        known_verbs[verb] = 'irregular'
    else:
      known_verbs[verb] = 'irregular'
  return known_verbs

  #this code creates our original regular verb dataframe
verb_types = get_verb_type(top_verbs)
for verb in verb_types:
    if verb_types[verb] == 'ar':
      verb_endings.loc[len(verb_endings)] = get_ar_endings(verb)
    elif verb_types[verb] == 'er':
      verb_endings.loc[len(verb_endings)] = get_er_endings(verb)
    elif verb_types[verb] == 'ir':
      verb_endings.loc[len(verb_endings)] = get_ir_endings(verb)
    else:
      verb_endings.loc[len(verb_endings)] = {'Infinitive': verb}

# Creates regular and irregular data frames
conj_cols = verb_endings.columns[1:]
regular_verbs = verb_endings[verb_endings[conj_cols].notna().all(axis=1)]
irregular_verbs = pd.DataFrame({
    'Infinitive': irregular_verbs_list
})
#and now the dataframes are concatenated to create our overall dictionary
#of our most common verbs in the past tense
#^ these will be the verbs that our script will be able to supply full info for!
verb_totals = pd.concat([regular_verbs, irregular_verbs])

person = {"1":"1st Person","2":"2nd Person","3":"3rd Person"} #<- person dictionary
number = {"SG":"Singular","PL":"Plural"} #<- plurality dictionary
formality = {"Inf":"Informal","For":"Formal"} #<- formality dictionary
tense = {"Simple":"Past Simple","Imperfect":"Past Imperfect"} #<- tense dictionary

def expand(col):
  """
  This function turns our verb_totals columns from the abreviations into full phrases
  """
  if col=="Infinitive":
    return col
  print(col)
  p,n,f,t = re.match(r"([123])(SG|PL)-(Inf|For)?PST(Simple|Imperfect)", col).groups()
  return f"{person[p]} {number[n]} {formality.get(f,'')} — {tense[t]}".replace("  "," ").strip()

verb_totals.columns = [expand(col) for col in verb_totals.columns] #<- replaces column names with expanded names, also helps out if we need to add more in the future

#this pair of functions is able to identify the words in our corpus and tokenize
#them using the open source package SpaCy which also has a pre trained portuguese
#package called pt_core_news_sm
nlp = spacy.load("pt_core_news_sm") #<- loads our pretrained portuguese nlp
regular_endings = ("ar", "er", "ir")
irregular = {"ser","estar","ter","ir","vir","pôr","dizer","fazer","haver","ver","querer","saber"}
text = cleaned_dialogue
verbs = []


def classify_verb(lemma):
  """
  This function identifies irregular and regular verbs
  """
  if lemma in irregular_verbs:
        return "irregular"
  if lemma.endswith(regular_endings):
        return "regular"
  return "unknown"

def get_past_tense(text):
  """
  This function identifies verbs in our corpus that are in the past tense
  """
  doc = nlp(text)
  results = []
  for token in doc:
    if token.pos_ == "VERB":
      lemma = token.lemma_
      classification = classify_verb(lemma)
      results.append({
        "token": token.text,
            "lemma": lemma,
              "classification": classification,
              "pos": token.pos
      })
  df = pd.DataFrame(results)
  return df

df_verbs = get_past_tense(cleaned_dialogue) #<- creates a dataframe of all the verbs in our corpus

def find_conjugation(token, lemma, verb_totals):
  """
  This function finds the conjugation of our known verbs if they are in the corpus
  """
  results = [] #<- initiating our list of identified conjugations (column titles)
  rows = verb_totals[verb_totals["Infinitive"] == lemma]
  for idx, row in rows.iterrows():
      cols = [col for col in verb_totals.columns if row[col] == token]
      if cols:
          results.append({
              "row_index": idx,
              "columns": cols
          })
  return results if results else None

def annotate_conjugations(df_verbs, verb_totals):
  """
  This function annotates our verbs with their conjugations, adding a column called conjugations to df_verbs
  """
  all_conjugations = []

  for _, row in df_verbs.iterrows():
      token = row["token"]
      lemma = row["lemma"]

      conjugations = find_conjugation(token, lemma, verb_totals)
      all_conjugations.append(conjugations)

  df_verbs["conjugations"] = all_conjugations
  return df_verbs

df_verbs = annotate_conjugations(get_past_tense(cleaned_dialogue), verb_totals) #<- df verbs now has additional column, conjugations

def interactive_piece(verbs_df, sentence):
    """
    Identifies the lemma, classification, conjugation (if known),
    and spaCy morphological features for each verb in a novel sentence.
    """
    results = []
    doc = nlp(sentence)

    for token in doc:
        if token.pos_ == "VERB" or token.pos_ == "AUX":
            word_lower = token.text.lower()

            # Try to match the verb in the known corpus
            match = verbs_df[verbs_df['token'].str.lower() == word_lower]

            if not match.empty:
                row = match.iloc[0]
                conjug_info = row['conjugations']

                if conjug_info is None:
                    conjugation_annotation = None
                else:
                    conjugation_annotation = [d['columns'][0] for d in conjug_info]
            else:
                # Verb is not in the corpus
                conjugation_annotation = None
                row = None

            # Extract spaCy morphological information
            morph = token.morph
            spaCy_features = {
                'Tense': morph.get('Tense'),
                'Person': morph.get('Person'),
                'Number': morph.get('Number'),
                'Mood': morph.get('Mood'),
                'VerbForm': morph.get('VerbForm')
            }

            results.append({
                'Verb': token.text,
                'Infinitive:': token.lemma_,
                'Classification': classify_verb(token.lemma_),
                'Conjugation Type': conjugation_annotation,
                'Morphology Features': spaCy_features
            })

    return results
def run_interactive_piece():
  """
  This function runs the interactive piece of the script
  """
  sentence = input("Enter a Portuguese sentence: ").strip()
  results = interactive_piece(df_verbs, sentence)
  print("\n--- Verb Info: ---")
  for r in results:
      print(r)

run_interactive_piece()